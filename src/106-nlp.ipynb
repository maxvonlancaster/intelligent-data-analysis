{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a6ceac",
   "metadata": {},
   "source": [
    "# Обробка Природної Мови (Natural Language Processing, NLP)\n",
    "\n",
    "Велика кількість даних у багатьох реальних наборах даних представлена у вигляді вільного тексту (free text) \n",
    "(коментарі користувачів, а також будь-які «неструктуровані» поля).\n",
    "\n",
    "(Обчислювальна) обробка природної мови: ми пишемо комп'ютерні програми, які можуть \n",
    "розуміти природну мову.\n",
    "\n",
    "Ця лекція: ми спробуємо отримати деяку значущу інформацію з неструктурованих текстових даних."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd04331c",
   "metadata": {},
   "source": [
    "## Основи обробки тексту\n",
    "\n",
    "Обробка тексту є важливим етапом підготовки текстових даних для завдань обробки природної мови (NLP). Вона передбачає перетворення необробленого тексту в структурований формат, який може ефективно використовуватися моделями машинного навчання.\n",
    "\n",
    "**Очищення тексту**\n",
    "\n",
    "- Переведення в нижній регістр: перетворює всі символи в тексті в нижній регістр для забезпечення однорідності.\n",
    "- Видалення розділових знаків: видаляє розділові знаки, які не впливають на значення тексту.\n",
    "- Видалення стоп-слів: видаляє часто вживані слова (наприклад, «і», «є», «в»), які не несуть значущого значення.\n",
    "- Видалення спеціальних символів і цифр: видаляє спеціальні символи і цифри, які часто не мають значення для текстового аналізу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e4a067c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vmelnyk2\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello example text numbers punctuation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vmelnyk2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in word_tokenize(text) if word not in stop_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "sample_text = \"Hello! This is an example text with numbers 123 and punctuation!!!\"\n",
    "cleaned_text = clean_text(sample_text)\n",
    "print(cleaned_text)  # Output: hello example text numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c11f7",
   "metadata": {},
   "source": [
    "**Токенізація (tokenization)**\n",
    "\n",
    "Токенізація — це процес розбиття тексту на менші одиниці, зазвичай слова або речення. Вона допомагає розбивати текстові дані на зручні для подальшої обробки частини.\n",
    "\n",
    "- Токенізація слів: розбиває текст на окремі слова.\n",
    "- Токенізація речень: розбиває текст на речення."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7833405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'exciting', '!', 'It', 'enables', 'computers', 'to', 'understand', 'human', 'language', '.']\n",
      "Sentences: ['Natural Language Processing (NLP) is exciting!', 'It enables computers to understand human language.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # Word tokenization\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Sentence tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    return words, sentences\n",
    "\n",
    "sample_text = \"Natural Language Processing (NLP) is exciting! It enables computers to understand human language.\"\n",
    "words, sentences = tokenize_text(sample_text)\n",
    "print(\"Words:\", words)\n",
    "print(\"Sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460740fc",
   "metadata": {},
   "source": [
    "**Стемінг та лематизація (Stemming and Lemmatization)**\n",
    "\n",
    "- Стемінг: скорочує слова до їхньої базової або кореневої форми шляхом видалення суфіксів (наприклад, «running» стає «run»). Стемінг може призвести до появи слів, яких немає в словнику.\n",
    "\n",
    "- Лематизація: скорочує слова до їхньої основи або форми, що є в словнику (лема), з урахуванням контексту (наприклад, «running» стає «run», але «better» стає «good»)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51557a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vmelnyk2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vmelnyk2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['the', 'children', 'are', 'play', 'with', 'their', 'toy', '.']\n",
      "Lemmatized Words: ['The', 'child', 'are', 'playing', 'with', 'their', 'toy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def apply_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Apply stemming\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return stemmed_words\n",
    "\n",
    "def apply_lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Apply lemmatization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return lemmatized_words\n",
    "\n",
    "sample_text = \"The children are playing with their toys.\"\n",
    "stemmed_words = apply_stemming(sample_text)\n",
    "lemmatized_words = apply_lemmatization(sample_text)\n",
    "print(\"Stemmed Words:\", stemmed_words)  # Output: ['the', 'children', 'are', 'play', 'with', 'their', 'toy']\n",
    "print(\"Lemmatized Words:\", lemmatized_words)  # Output: ['the', 'child', 'are', 'playing', 'with', 'their', 'toy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e836f",
   "metadata": {},
   "source": [
    "**Методи нормалізації**\n",
    "\n",
    "Методи нормалізації стандартизують текстові дані, перетворюючи їх у єдиний формат. Це може включати:\n",
    "\n",
    "- Обробка синонімів: перетворення синонімів у загальне представлення (наприклад, «автомобіль» і «машина» стають «автомобілем»).\n",
    "- Обробка абревіатур: розширення абревіатур до повної форми (наприклад, «u» стає «you»).\n",
    "- Виправлення орфографічних помилок: виправлення слів з орфографічними помилками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "705d9de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are the best  please help me with this assignment  thanks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vmelnyk2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from autocorrect import Speller\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Initialize spell checker\n",
    "    spell = Speller(lang='en')\n",
    "\n",
    "    # Define a dictionary for common abbreviations\n",
    "    normalization_dict = {\n",
    "        \"u\": \"you\",\n",
    "        \"ur\": \"your\",\n",
    "        \"r\": \"are\",\n",
    "        \"pls\": \"please\",\n",
    "        \"pos\": \"please\",\n",
    "        \"thx\": \"thanks\"\n",
    "    }\n",
    "\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Handle abbreviations, spelling corrections, and remove special characters\n",
    "    normalized_words = [normalization_dict.get(word, spell(word)) for word in words]\n",
    "\n",
    "    # Reconstruct the text\n",
    "    normalized_text = ' '.join(normalized_words)\n",
    "\n",
    "    # Clean special characters after spelling corrections\n",
    "    normalized_text = normalized_text.replace('!', '').replace('.', '').strip()\n",
    "\n",
    "    return normalized_text\n",
    "\n",
    "sample_text = \"U r the best! Pls help me with this assignment. Thx!\"\n",
    "normalized_text = normalize_text(sample_text)\n",
    "print(normalized_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
